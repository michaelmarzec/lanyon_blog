<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Michael Marzec</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2022-02-08T16:48:26-07:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>Spotify Playlist Clutering: Exploratory Analysis</title>
   <link href="http://localhost:4000/spotifyplaylistclustering-exploratoryanalysis"/>
   <updated>2022-02-07T00:00:00-07:00</updated>
   <id>http://localhost:4000/spotifyplaylistclustering-exploratoryanalysis</id>
   <content type="html">&lt;p&gt;I love the ease of adding a song to my Spotify ‘Liked Songs’, but I am admittedly too lazy to create and update playlists. This is fine until I want to shuffle my ‘Liked Songs’ and suddenly my playlist jumps from Prince to Leon Bridges to Spoon.&lt;/p&gt;

&lt;p&gt;Spotify’s playlist genre filters offer a quick solution, but genre filters can lack a certain nuance. For example, while an album can be classified as “pop” its specific singles can feature not only a variety of high-energy workout music, but background working music, and also something to play during a family dinner. Some artists are more inclined to this than others (e.g., James Blake, Beck, Kanye West), however, regardless of the artist or genre, I’d like to apply this concept to my entire ‘Liked Songs’ library and create customized playlists.&lt;/p&gt;

&lt;h4 id=&quot;data&quot;&gt;Data&lt;/h4&gt;

&lt;p&gt;Spotify provides a fantastic &lt;ins&gt;&lt;a href=&quot;https://developer.spotify.com/documentation/web-api/&quot;&gt;API&lt;/a&gt;&lt;/ins&gt; to access their data and &lt;ins&gt;&lt;a href=&quot;https://spotipy.readthedocs.io/en/2.19.0/&quot;&gt;Spotipy&lt;/a&gt;&lt;/ins&gt; makes it very easy to use. The API provides three key features:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Download your ‘Liked Songs’.&lt;/li&gt;
  &lt;li&gt;Extract key features for each song.&lt;/li&gt;
  &lt;li&gt;Create and populate playlists.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first and third points are fairly self-explanatory, but the ‘key features’ is a custom Spotify API output. This includes characteristics like ‘danceability’, ’energy’, and ‘tempo’ among others listed &lt;ins&gt;&lt;a href=&quot;https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features&quot;&gt;here&lt;/a&gt;&lt;/ins&gt;. Thus, using these API functions, I can easily download a list of every liked song, about 12 descriptive variables for each song, and ultimately, group these songs into new playlists.&lt;/p&gt;

&lt;p&gt;In my opinion, this solution’s most significant assumption is its reliance on Spotify’s pre-determined audio features. While they are convenient and easy to use, the solution is dependent on Spotify’s calculated audio features reliably encapsulating each song’s, for lack of a better word, vibe. Essentially, the audio features provide a great foundation for this project, but given the appropriate access, I would love to leverage audio files (e.g., .wav file) to analyze the raw audio data via a library such as &lt;ins&gt;&lt;a href=&quot;https://librosa.org/doc/latest/index.html&quot;&gt;Librosa&lt;/a&gt;&lt;/ins&gt;.&lt;/p&gt;

&lt;h4 id=&quot;pca&quot;&gt;PCA&lt;/h4&gt;

&lt;p&gt;After extracting the descriptive variables, I standardized the data and applied principal component analysis (PCA), which among other benefits, reduces dimensionality and the risk of multicollinearity.&lt;/p&gt;

&lt;p&gt;When using PCA, the end-user needs to determine the number of ‘principal components’ to keep for any subsequent processing. There are a couple of typical options:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Visualize the principal components using a scree plot and determine the appropriate number to keep via ‘elbow’ analysis.&lt;/li&gt;
  &lt;li&gt;Determine the preferred variance to ‘capture’ via the principal components and let that determine the number to keep.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the following for a scree plot example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/Spotify-Playlist-Classification/pca_screeplot.png&quot; style=&quot;float: center; width: 450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Elbow analysis suggests using an inflection point of significant change. At first glance, this suggests using two principal components. There is also precedence to using a ‘variance threshold’ approach. In later sections, I’ll discuss my evaluation methodology, which includes models that use two principal components versus a variance threshold.&lt;/p&gt;

&lt;p&gt;In addition, note that a customized environment often provides the opportunity to evaluate a scree plot and subsequently determine the optimal number of components. However, by choosing a variance threshold, the solution can be simplified and thus better prepared for scalable deployment.&lt;/p&gt;

&lt;h4 id=&quot;clustering&quot;&gt;Clustering&lt;/h4&gt;

&lt;p&gt;After selecting the principal components, this data can be fed to a clustering algorithm. The algorithm will separate each data point (i.e., song) into a different cluster. In this instance, each cluster will represent a playlist. I considered two approaches to determining the number of clusters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Non-Technical
    &lt;ul&gt;
      &lt;li&gt;My ‘Liked Songs’ playlist consists of roughly 1500 songs. At 75 songs per playlist, I would specify 20 clusters. While there is nothing technical to this approach, 75 songs per playlist strikes me as a healthy balance between the number of playlists and song per playlist.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Technical
    &lt;ul&gt;
      &lt;li&gt;The appropriate number of clusters can be analyzed via an inertia plot. Similar to PCA, it’s an inexact science that commonly uses the elbow method.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the following inertia plot, I selected 11 clusters (which frankly feels like an arbitrary decision) as my ‘technical’ approach fur further evaluation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/Spotify-Playlist-Classification/cluster_inertiaplot.png&quot; style=&quot;float: center; width: 450px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that so far I’ve avoided mentioning which clustering algorithm is being used. To this point, I’ve used &lt;ins&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html&quot;&gt;k-means clustering via scikit-learn library&lt;/a&gt;&lt;/ins&gt;. There are countless clustering algorithms, each of which uses another variation to achieve the same objective: sort each data point into groups with other similar data points.&lt;/p&gt;

&lt;p&gt;In the following evaluation section, I experiment with k-means, birch, dbscan, and gaussian clustering. The experiment results highlight some differences across each algorithm. However, the following graphic is a personal favorite which visualizes the results of a handful of clustering algorithms based on the data distribution:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68&quot;&gt;
&lt;img src=&quot;./assets/Spotify-Playlist-Classification/clustering_algos.png&quot; style=&quot;float: center&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h4&gt;

&lt;p&gt;Evaluating unsupervised exploratory analysis is inherently difficult. There are no “correct” answers to measure the results against (i.e., the Y / independent variable). I can evaluate the playlists using personal judgment, but any outcome assessment is subjective at best.&lt;/p&gt;

&lt;p&gt;Altogether, I ran eight versions with the following parameters:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/Spotify-Playlist-Classification/table.png&quot; style=&quot;float: center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, using clustering maps based on two principal components, the dbscan output stands out. This algorithm determines the number of clusters rather than allowing the user to decide. Per the following plot, there are only three clusters, and the patterns differ significantly from the other algorithms. For those reasons, I’ve ruled this out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/Spotify-Playlist-Classification/v4_cluster_scatter_plot.png&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, we can see very similar cluster patterns applied across the remaining algorithms:&lt;/p&gt;

&lt;table&gt;&lt;tr&gt;
&lt;td&gt; &lt;img src=&quot;./assets/Spotify-Playlist-Classification/v2_cluster_scatter_plot.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 250px;&quot; /&gt; &lt;/td&gt;
&lt;td&gt; &lt;img src=&quot;./assets/Spotify-Playlist-Classification/v3_cluster_scatter_plot.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 250px;&quot; /&gt; &lt;/td&gt;
&lt;td&gt; &lt;img src=&quot;./assets/Spotify-Playlist-Classification/v5_cluster_scatter_plot.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 250px;&quot; /&gt; &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Considering the similarity in the graphs, I’m going to move forward with the k-means algorithm (V2). The following shows 11 vs 20 playlists in a cluster:&lt;/p&gt;

&lt;table&gt;&lt;tr&gt;
&lt;td&gt; &lt;img src=&quot;./assets/Spotify-Playlist-Classification/v2_cluster_scatter_plot.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 350px;&quot; /&gt; &lt;/td&gt;
&lt;td&gt; &lt;img src=&quot;./assets/Spotify-Playlist-Classification/v6_cluster_scatter_plot.png&quot; alt=&quot;Drawing&quot; style=&quot;width: 350px;&quot; /&gt; &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;In all honesty, the cluster images don’t leave me with a strong preference. There are a lot of similar groupings with slightly more granular breakdowns. As such, I’m going to move forward with 20 playlists (i.e., my non-technical approach) and, as mentioned, I’d prefer to use a variance threshold (rather than an ad-hoc principal component decision). This leaves me with V7 as my final model to create my customized Spotify playlists.&lt;/p&gt;

&lt;h4 id=&quot;conclusion-and-future-efforts&quot;&gt;Conclusion and Future Efforts&lt;/h4&gt;

&lt;p&gt;Altogether, I am happy with my first attempt at automated playlists. The playlists aren’t perfect (to be expected) but they seem to have some coherent similarities. A few additional concluding thoughts and ideas for future updates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I would like to add an output report (or title the playlist) based on the various audio features. For example, whichever playlist has the highest average ‘danceability’ could be titled “autoPlaylist_dancetracks’.&lt;/li&gt;
  &lt;li&gt;I want to develop end-to-end deployment for a casual end-user. At a bare minimum, the parameters could be limited to the Spotify Client ID, Spotify Secret Code, Spotify username, and number of desired playlists. For advanced users, additional options could include principal component threshold, preferred clustering algorithm, and whether to print the scree and inertia plots.
    &lt;ul&gt;
      &lt;li&gt;Ideally, this could be completed via a simple hosted website or a local executable.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;I am still considering what a model rerun should look like… Should it retain the original model and predict (i.e., place) the newly liked songs into the existing playlists? Or should it delete all cluster-based playlists, retrain and recreate the playlists from scratch?
    &lt;ul&gt;
      &lt;li&gt;In a functional application, this could be another advanced option for technical users with ‘delete and recreate’ as a default option.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;As mentioned, I would like to experiment with changing the input source to .wav/librosa data to remove dependency on Spotify’s audio feature research for comparison’s sake. Obtaining this data is probably unrealistic but still an interesting thought for future work.&lt;/li&gt;
  &lt;li&gt;If you have any recommendations, questions, comments, etc. please feel free to reach out via email!&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>NBA-Age Analysis: Infrastructure</title>
   <link href="http://localhost:4000/nbaageanalysis-infrastructure"/>
   <updated>2022-02-01T00:00:00-07:00</updated>
   <id>http://localhost:4000/nbaageanalysis-infrastructure</id>
   <content type="html">&lt;h3 style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://nbaage.com/&quot;&gt;&lt;ins&gt;nbaage.com&lt;/ins&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;When analysts, media members, and fans discuss the average age of an NBA team, there’s often a missing nuance. Typically, it’s a reference like “Team X is the 5th youngest team in the NBA” and that marker results from a quick average of every roster member. In my opinion, this fails to convey an accurate representation as that average is skewed by low usage and end-of-bench players.&lt;/p&gt;

&lt;h4 id=&quot;information-objective&quot;&gt;Information Objective&lt;/h4&gt;

&lt;p&gt;Present an innovative view of NBA ages. Rather than convey that the Minnesota Timberwolves are the 7th youngest team, I’d like to explain that the Timberwolves are the 5th youngest team when weighted by minutes and the 4th youngest team when weighted by usage rate. This could indicate that the Wolves rely on young players surrounded by older low-usage veterans. We can also see that their team-wide average is pulled up by the ‘Combo’ position (i.e., Patrick Beverly). Over a full season, perhaps we can identify inflection points such as key injuries or when a team decides to ‘tank.’&lt;/p&gt;

&lt;h4 id=&quot;infrastructure-objective&quot;&gt;Infrastructure Objective&lt;/h4&gt;

&lt;p&gt;Develop a tech stack that will (for cheap, if not free) provide a clean, easy-to-interpret, and automated presentation of NBA team ages. This infrastructure should reduce, if not eliminate, manual upkeep, allowing for time spent analyzing the data and enhancing website functionality.&lt;/p&gt;

&lt;h4 id=&quot;data-extraction&quot;&gt;Data Extraction&lt;/h4&gt;

&lt;p&gt;All data is extracted (nightly) using Python (BeautifulSoup, UrlLib, etc.), automated via AWS Lambda, stored in S3, and scraped from Ben Falk’s &lt;ins&gt;&lt;a href=&quot;https://cleaningtheglass.com/&quot;&gt;cleaningtheglass.com&lt;/a&gt;&lt;/ins&gt;. A couple of lessons learned:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When developing an AWS Lambda function, keep it lightweight! There is a compressed limit of 50mb.&lt;/li&gt;
  &lt;li&gt;In this instance, I adjusted from storing data in a Parquet file (my preferred local storage method) to a CSV, so I could remove the PyArrow library.&lt;/li&gt;
  &lt;li&gt;&lt;ins&gt;&lt;a href=&quot;https://github.com/keithrozario/Klayers/blob/master/deployments/python3.8/arns/us-east-2.csv&quot;&gt;Linked&lt;/a&gt;&lt;/ins&gt; is a fantastic resource for Python ARNs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;front-end&quot;&gt;Front End&lt;/h4&gt;

&lt;p&gt;The web framework is developed using the Flask library. Data is ingested from S3 (i.e., Boto3), which conveniently creates an automated pipeline due to the Lambda function. The data is manipulated via a handful of Pandas operations, which feed the &lt;ins&gt;&lt;em&gt;&lt;a href=&quot;https://flask-table.readthedocs.io/en/stable/#sortable-tables&quot;&gt;sortable&lt;/a&gt;&lt;/em&gt;&lt;/ins&gt; data table and the time-series data to HTML templates.&lt;/p&gt;

&lt;p&gt;The HTML table template uses a basic structure and a simple CSS file (i.e., the bulk of the research/work was completed via the Flask-Table library).&lt;/p&gt;

&lt;p&gt;The HTML time-series template uses chart.js and a slightly more complex CSS file:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I manually created a json set for each team which strikes me as an opportunity for improvement via some type of an automated for-loop solution.&lt;/li&gt;
  &lt;li&gt;My flex-wrapper class was a lifesaver. I struggled to station the footer below the chart, but the combination of ‘display’, ‘flex-direction’, and ‘justify-content’ resolved the issue.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;deployment&quot;&gt;Deployment&lt;/h4&gt;

&lt;p&gt;This website might not exist without &lt;ins&gt;&lt;a href=&quot;https://github.com/zappa/Zappa&quot;&gt;Zappa&lt;/a&gt;&lt;/ins&gt;. For anyone unfamiliar, the library uses AWS Lambda (as such, this website leverages Lambda twice … perhaps another point for improvement) to serve a Python application and connect it via Amazon’s API Gateway. Zappa’s user interface requests a few basic parameters and a quick Google will provide you with helpful hints like leveraging the pre-built slim_handler, using the AWS us-east-2 region, and the ‘zappa tail’ function for troubleshooting.&lt;/p&gt;

&lt;p&gt;In addition, Zappa allows for custom domains. The following is a quick walkthrough for any newcomers that would like to consolidate the research process:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I love &lt;ins&gt;&lt;a href=&quot;https://domains.google.com/&quot;&gt;Google Domains&lt;/a&gt;&lt;/ins&gt;. Accessible interface, simple to maintain and you don’t have to manage another new website account. Buy your favorite domain and continue.&lt;/li&gt;
  &lt;li&gt;In the AWS Certificate Manager, request a ‘Public Certificate’, enter your domain name, and request a DNS validation. Once this is available, select the option to ‘Create records in Route 53.’&lt;/li&gt;
  &lt;li&gt;In AWS Route 53, you should now have an NS-Type record name for your domain. Copy the four NS ‘routes’ (e.g., ns-1000.awsdns-11.org.), open the DNS:Custom Name Servers tab in Google Domains, and save the four different NS values.&lt;/li&gt;
  &lt;li&gt;Your custom domain should be active now! You just need the ARN value from your AWS Certificate page. In your zappa_settings.json file add the following:
“certificate_arn”: “arn_value”
“Domain”:”custom_domain”&lt;/li&gt;
  &lt;li&gt;Run a zappa update &lt;em&gt;project name&lt;/em&gt; and your domain should be live!&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;sign-off&quot;&gt;Sign Off&lt;/h4&gt;

&lt;p&gt;For anyone who is still reading, I hope this helped. Please feel free to reach out with any questions, comments, or ideas for &lt;ins&gt;&lt;a href=&quot;https://nbaage.com/&quot;&gt;nbaage.com&lt;/a&gt;&lt;/ins&gt;! I intend for it to evolve, but we all know life can get in the way.&lt;/p&gt;

&lt;p&gt;Any significant infrastructure updates will be added here and I plan to post analysis using this data-–as usual, the data pipeline and presentation is just the first step. In addition, if you attempt to replicate this process and encounter any errors, shoot me an email and I’ll be happy to give you my best effort.&lt;/p&gt;
</content>
 </entry>
 

</feed>
